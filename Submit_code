import pandas as pd
import numpy as np
from scipy import stats
import pandas as pd
import numpy as np
from scipy import stats

data=pd.read_csv('C:/Users/Vivek/Desktop/Regression.csv')
data.head()
input	output
0	0	0.1
1	1	0.9
2	2	2.1
3	3	2.9
4	4	4.1

x=data['input'].values
y=data['output'].values
y_pred_1=[0, 1, 2, 3, 4]
y_pred_2=[0.1, 0.9, 2.1, 2.9, 4.1]

#finding mean
mean_x=np.mean(x)
mean_y=np.mean(y)
​
#total no. of values
m=len(x)
​
#calculate b1 and b0
numer=0
denom=0
for i in range(m):
    numer += (x[i]-mean_x)*(y[i]-mean_y)
    denom += (x[i]-mean_x)**2             #squaring
b1=numer/denom
b0=mean_y - (b1*mean_x)
​
print(b1,b0)
1.0 0.020000000000000018


m=len(x)
​
#Finding the RMSE value for Model 1
​
rmse1=0
for i in range(m):
    rmse1 += (y[i]-y_pred_1[i])**2
rmse1=np.sqrt(rmse1/m)
print("RMSE for Model 1 is",rmse1)
RMSE for Model 1 is 0.09999999999999996

#Finding the RMSE value for Model 2
rmse2=0
for i in range(m):
    rmse2 += (y[i]-y_pred_2[i])**2
rmse2=np.sqrt(rmse2/m)
print("RMSE for Model 2 is",rmse2)
RMSE for Model 2 is 0.0

The closer the value of RMSE is to zero , the better is the Regression Model. So, model 2 is better.



#measuring accuracy using rsquare score for model 1
​
ss_t=0   #total sum of square
ss_r=0   #total sum of square of residuals
for i in range(m):
    ss_t += (y[i]-mean_y)**2
    ss_r += (y[i]-y_pred_1[i])**2
r2_model1=1-(ss_r/ss_t)
print(r2_model1)
0.9950238853503185

#measuring accuracy using rsquare score for model 2
​
ss_t=0   #total sum of square
ss_r=0   #total sum of square of residuals
for i in range(m):
    ss_t += (y[i]-mean_y)**2
    ss_r += (y[i]-y_pred_2[i])**2
r2_model2=1-(ss_r/ss_t)
print(r2_model2)
1.0
​
